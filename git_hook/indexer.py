import os
import shutil
import chromadb
from typing import List
from tqdm import tqdm
from zhipuai import ZhipuAI

# LangChain ç»„ä»¶
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import LanguageParser
from langchain_text_splitters import Language, RecursiveCharacterTextSplitter

# Configuration
REPO_PATH = "."  
DB_PATH = "./chroma_db"
API_KEY = os.getenv("MEDICAL_RAG") 

# --- æ ¸å¿ƒé…ç½® ---
LANGUAGE_MAP = {
    ".py":   (Language.PYTHON, "repo_python"),
    ".java": (Language.JAVA,   "repo_java"),
    ".js":   (Language.JS,     "repo_js"),
    ".ts":   (Language.TS,     "repo_js"), 
    ".html": (Language.HTML,   "repo_html"), # HTML åœ¨ Parser é˜¶æ®µä¼šé™çº§å¤„ç†
    ".go":   (Language.GO,     "repo_go"),
    ".cpp":  (Language.CPP,    "repo_cpp"),
}

class ZhipuEmbeddingFunction(chromadb.EmbeddingFunction):
    def __init__(self, api_key):
        self.api_key = api_key
        self.client = ZhipuAI(api_key=api_key)

    def __call__(self, input: List[str]) -> List[List[float]]:
        response = self.client.embeddings.create(model="embedding-3", input=input)
        return [data.embedding for data in response.data]

def build_index():
    if not API_KEY:
        raise ValueError("âŒ Error: API Key environment variable is missing.")

    print(f"ğŸš€ Starting Multi-Language Indexing for: {os.path.abspath(REPO_PATH)}")

    if os.path.exists(DB_PATH):
        try:
            shutil.rmtree(DB_PATH)
            print(f"ğŸ§¹ Cleaned up existing DB at {DB_PATH}")
        except Exception as e:
            print(f"âš ï¸ Warning: Could not delete old DB: {e}")

    client = chromadb.PersistentClient(path=DB_PATH)
    emb_fn = ZhipuEmbeddingFunction(api_key=API_KEY)

    # --- å¾ªç¯å¤„ç†æ¯ç§è¯­è¨€ ---
    for suffix, (lang_enum, collection_name) in LANGUAGE_MAP.items():
        print(f"\nğŸ“‚ Processing {suffix} files for collection: '{collection_name}'...")

        # --- A. æ™ºèƒ½åŠ è½½ (Load) ---
        # å¢åŠ å®¹é”™é€»è¾‘ï¼šå¦‚æœ LanguageParser ä¸æ”¯æŒè¯¥è¯­è¨€(å¦‚ HTML)ï¼Œåˆ™å›é€€åˆ°é»˜è®¤åŠ è½½å™¨
        parser = None
        try:
            # å°è¯•åˆå§‹åŒ–é«˜çº§è§£æå™¨
            parser = LanguageParser(language=lang_enum, parser_threshold=500)
        except Exception:
            # æ•è· "No parser available" é”™è¯¯
            print(f"   â„¹ï¸  Note: Advanced parser not available for {suffix}. Using basic text loader.")
            parser = None

        # æ ¹æ®æ˜¯å¦æˆåŠŸåˆå§‹åŒ– parser æ¥å†³å®šåŠ è½½æ–¹å¼
        if parser:
            loader = GenericLoader.from_filesystem(
                REPO_PATH,
                glob=f"**/*{suffix}",
                parser=parser
            )
        else:
            # å¦‚æœ parser ä¸ºç©ºï¼ˆä¾‹å¦‚ HTMLï¼‰ï¼Œä¸ä¼  parser å‚æ•°ï¼ŒGenericLoader ä¼šé»˜è®¤æŒ‰æ–‡æœ¬/MIMEå¤„ç†
            loader = GenericLoader.from_filesystem(
                REPO_PATH,
                glob=f"**/*{suffix}"
            )

        documents = loader.load()
        
        if not documents:
            print(f"   âš ï¸ No {suffix} files found. Skipping.")
            continue

        print(f"   ğŸ“„ Loaded {len(documents)} {suffix} documents.")

        # --- B. åˆ‡åˆ† (Split) ---
        # å³ä½¿ Parser å¤±è´¥ï¼ŒSplitter ä¾ç„¶æ”¯æŒ HTML ç­‰è¯­è¨€çš„æ­£åˆ™åˆ‡åˆ†ï¼Œè¿™å¾ˆå¥½
        splitter = RecursiveCharacterTextSplitter.from_language(
            language=lang_enum, 
            chunk_size=1000, 
            chunk_overlap=200
        )
        split_docs = splitter.split_documents(documents)
        print(f"   ğŸ§© Split into {len(split_docs)} chunks.")

        # --- C. å­˜å‚¨ (Store) ---
        collection = client.get_or_create_collection(
            name=collection_name,
            embedding_function=emb_fn
        )

        batch_size = 64
        total_docs = len(split_docs)
        
        for start_idx in range(0, total_docs, batch_size):
            end_idx = min(start_idx + batch_size, total_docs)
            batch = split_docs[start_idx:end_idx]
            
            batch_ids = []
            batch_documents = []
            batch_metadatas = []
            
            for i, doc in enumerate(batch):
                batch_documents.append(doc.page_content)
                
                meta = doc.metadata.copy()
                meta["language"] = str(lang_enum) 
                for k, v in meta.items():
                    if v is None: meta[k] = ""
                batch_metadatas.append(meta)
                
                safe_name = os.path.basename(meta.get('source', 'unknown')).replace('.', '_')
                unique_id = f"{collection_name}_{safe_name}_{start_idx + i}"
                batch_ids.append(unique_id)
            
            if batch_ids:
                collection.add(ids=batch_ids, documents=batch_documents, metadatas=batch_metadatas)

    print(f"\nâœ… All Languages Indexed Successfully into {DB_PATH}")

if __name__ == "__main__":
    build_index()